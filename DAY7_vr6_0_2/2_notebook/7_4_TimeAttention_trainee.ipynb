{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeAttentionレイヤを実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* 以下のWeightSum, AttentionWeight, Attention, TimeAttentionクラスを完成させましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        hs : エンコーダの中間状態\n",
    "        a : アテンション荷重\n",
    "        \"\"\"\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        # アテンション荷重の行列を3次元配列に変形する\n",
    "        ar = a.reshape(N, T, 1)#.repeat(H, axis=2)   ブロードキャストを明示的に行いたい場合はrepeatを付ける\n",
    "        # エンコーダの中間状態にアテンション荷重をかけて、それを足し合わせることによって、加重平均を求める\n",
    "        t =       *                                                                                                                                                                                # <- 穴埋め\n",
    "        c = np.sum(      ,axis=  )                                                                                                                                                      # <- 穴埋め\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c  # エンコーダの中間状態を加重平均した結果\n",
    "\n",
    "    def backward(self, dc):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da\n",
    "\n",
    "\n",
    "class AttentionWeight:\n",
    "    \"\"\"\n",
    "    アテンション荷重を算出するクラス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        アテンション荷重を求める\n",
    "        hs : エンコーダの全ての中間状態\n",
    "        h : デコーダのある場所の中間状態\n",
    "        \"\"\"\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        #　デコーダのある場所の中間状態を3次元配列に変形する\n",
    "        hr = h.reshape(N, 1, H)\n",
    "        \n",
    "        # エンコーダの中間状態とデコーダの中間状態を掛けて足し合わせることで内積をとる\n",
    "        # 他の実装例として、hsとhrを結合し、重みWを掛けるという方法もある\n",
    "        t = hs * hr\n",
    "        s = np.sum(        , axis=          )                                                                                                                                                      # <- 穴埋め\n",
    "        \n",
    "        # ソフトマックス関数に通すことで、正規化する\n",
    "        a = self.softmax.forward(s) # アテンション重みベクトルを並べた行列 (N * T)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class Attention:\n",
    "    \"\"\"\n",
    "    アテンション\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        \n",
    "        # レイヤの定義\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        hs : エンコーダの中間状態\n",
    "        h : デコーダの中間状態\n",
    "        \"\"\"\n",
    "        # アテンション荷重を求める\n",
    "        a = self.attention_weight_layer.forward(        ,     )                                                                                                                   # <- 穴埋め\n",
    "        \n",
    "        # エンコーダの中間状態にアテンション荷重をかける\n",
    "        out = self.weight_sum_layer.forward(     ,      )                                                                                                                      # <- 穴埋め\n",
    "        self.attention_weight = a\n",
    "        \n",
    "        return out # エンコーダの中間状態を加重平均した結果\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        \"\"\"\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh\n",
    "\n",
    "\n",
    "class TimeAttention:\n",
    "    \"\"\"\n",
    "    アテンションレイヤを時間方向にまとめるレイヤ\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        hs_enc : エンコーダの中間状態\n",
    "        hs_dec : デンコーダの中間状態\n",
    "        \"\"\"\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(   ):\n",
    "            \"\"\"\n",
    "            出力単語数分を繰り返す\n",
    "            \"\"\"\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(          , hs_dec[:,t,:])                                                                                                                 # <- 穴埋め\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        逆伝播\n",
    "        dout : 勾配\n",
    "        \"\"\"\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            \"\"\"\n",
    "            出力単語数分を繰り返す\n",
    "            \"\"\"\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中間層ノード数\n",
    "H = 4\n",
    "# データ数\n",
    "N = 3\n",
    "# 単語数\n",
    "T = 5\n",
    "\n",
    "\n",
    "# モデル構築\n",
    "ta = TimeAttention()\n",
    "\n",
    "hs_enc = np.random.randn(N*T*H).reshape(N, T, H)\n",
    "hs_dec =  np.random.randn(N*T*H).reshape(N, T, H)\n",
    "print(\"hs_enc=\", hs_enc)\n",
    "print()\n",
    "print(\"hs_dec=\", hs_dec)\n",
    "print()\n",
    "\n",
    "# 順伝播計算\n",
    "out = ta.forward(hs_enc, hs_dec)\n",
    "print(\"out=\", out)\n",
    "print()\n",
    "\n",
    "# 逆伝播計算\n",
    "dout = np.random.randn(N*T*H).reshape(N, T, H)\n",
    "dhs_enc, dhs_dec = ta.backward(dout)\n",
    "print(\"dhs_enc=\", dhs_enc)\n",
    "print()\n",
    "print(\"dhs_dec=\", dhs_dec)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
