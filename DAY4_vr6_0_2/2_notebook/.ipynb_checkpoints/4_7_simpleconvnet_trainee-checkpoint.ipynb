{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シンプルな畳み込みニューラルネットワークのクラスを実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from common.layers import Convolution, MaxPooling, ReLU, Affine, SoftmaxWithLoss\n",
    "from common.gradient import numerical_gradient\n",
    "from common.optimizer import RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* 以下のSimpleConvNetクラスを完成させましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 pool_param={'pool_size':2, 'pad':0, 'stride':2},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        \"\"\"\n",
    "        input_size : tuple, 入力の配列形状(チャンネル数、画像の高さ、画像の幅)\n",
    "        conv_param : dict, 畳み込みの条件\n",
    "        pool_param : dict, プーリングの条件\n",
    "        hidden_size : int, 隠れ層のノード数\n",
    "        output_size : int, 出力層のノード数\n",
    "        weight_init_std ： float, 重みWを初期化する際に用いる標準偏差\n",
    "        \"\"\"\n",
    "                \n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        \n",
    "        pool_size = pool_param['pool_size']\n",
    "        pool_pad = pool_param['pad']\n",
    "        pool_stride = pool_param['stride']\n",
    "        \n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size + 2*filter_pad - filter_size) // filter_stride + 1 # 畳み込み後のサイズ(H,W共通)\n",
    "        pool_output_size = (conv_output_size + 2*pool_pad - pool_size) // pool_stride + 1 # プーリング後のサイズ(H,W共通)\n",
    "        pool_output_pixel = filter_num * pool_output_size * pool_output_size # プーリング後のピクセル総数\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        std = weight_init_std\n",
    "        \n",
    "        # W1は畳み込みフィルターの重み \n",
    "        # 配列形状=(フィルター枚数, チャンネル数, フィルター高さ, フィルター幅)\n",
    "        self.params['W1'] =  std * np.random.randn(    ,     ,     ,    )                         # <- 穴埋め\n",
    "        \n",
    "        # b1は畳み込みフィルターのバイアス\n",
    "        # 配列形状=(フィルター枚数)\n",
    "        self.params['b1'] = np.zeros(     )                                                       # <- 穴埋め\n",
    "        \n",
    "        # 全結合層の重みW\n",
    "        # 配列形状=(前の層のノード数, 次の層のノード数)         \n",
    "        self.params['W2'] = std * np.random.randn(   ,   )                                        # <- 穴埋め\n",
    "        \n",
    "        # 全結合層のバイアスb\n",
    "        # 配列形状=(次の層のノード数)        \n",
    "        self.params['b2'] = np.zeros(     )                                                       # <- 穴埋め\n",
    "        \n",
    "        # 全結合層の重みW\n",
    "        # 配列形状=(前の層のノード数, 次の層のノード数)        \n",
    "        self.params['W3'] = std * np.random.randn(  ,    )                                        # <- 穴埋め\n",
    "        \n",
    "        # 全結合層のバイアスb\n",
    "        # 配列形状=(次の層のノード数)        \n",
    "        self.params['b3'] = np.zeros(     )                                                       # <- 穴埋め\n",
    "\n",
    "        \n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad']) # W1が畳み込みフィルターの重み, b1が畳み込みフィルターのバイアスになる\n",
    "        self.layers['ReLU1'] = ReLU()\n",
    "        self.layers['Pool1'] = MaxPooling(pool_h=pool_size, pool_w=pool_size, stride=pool_stride, pad=pool_pad)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['ReLU2'] = ReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        x : 入力データ\n",
    "        t : 教師データ\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝播法）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師データ\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNISTデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "train = X_train/255\n",
    "test = X_test/255\n",
    "train = train.reshape(-1, 28*28)\n",
    "test = test.reshape(-1, 28*28)\n",
    "train_labels = lb.fit_transform(y_train)\n",
    "test_labels = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配列形状の変形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.reshape(-1, 1, 28, 28)\n",
    "test = test.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ミニバッチ学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train[:1000,:]\n",
    "t = train_labels[:1000,:]\n",
    "\n",
    "x = x.reshape(-1,1,28,28) # 配列形式の変形\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "optimizer = RMSProp(lr=0.01, rho=0.9)\n",
    "\n",
    "# 繰り返し回数\n",
    "xsize = x.shape[0]\n",
    "iter_num = np.ceil(xsize / batch_size).astype(np.int)\n",
    "\n",
    "\n",
    "# CNNのオブジェクト生成\n",
    "snet = SimpleConvNet(input_dim=(1, 28, 28), \n",
    "                     conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                     pool_param={'pool_size':2, 'pad':0, 'stride':2},\n",
    "                     hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch=%s\"%epoch)\n",
    "\n",
    "    # シャッフル\n",
    "    idx = np.arange(xsize)\n",
    "    np.random.shuffle(idx)\n",
    "\n",
    "    for it in range(iter_num):\n",
    "        \"\"\"\n",
    "        ランダムなミニバッチを順番に取り出す\n",
    "        \"\"\"\n",
    "        print(\"it=\", it)\n",
    "        mask = idx[batch_size*it : batch_size*(it+1)]\n",
    "\n",
    "        # ミニバッチの生成\n",
    "        x_train = x[mask]\n",
    "        t_train = t[mask]\n",
    "\n",
    "        # 勾配の計算 (誤差逆伝播法を用いる) \n",
    "        grads = snet.gradient(x_train, t_train)\n",
    "\n",
    "        # 更新\n",
    "        optimizer.update(snet.params, grads)\n",
    "\n",
    "    ## 学習経過の記録\n",
    "\n",
    "    # 訓練データにおけるloss\n",
    "#     print(\"calculating train_loss\")    \n",
    "    train_loss.append(snet.loss(x,  t))\n",
    "\n",
    "#     print(\"calculating test_loss\")\n",
    "    # テストデータにおけるloss\n",
    "    test_loss.append(snet.loss(test, test_labels))\n",
    "\n",
    "#     print(\"calculating train_accuracy\")\n",
    "    # 訓練データにて精度を確認\n",
    "    train_accuracy.append(snet.accuracy(x, t))\n",
    "    \n",
    "#     print(\"calculating test_accuracy\")\n",
    "    # テストデータにて精度を算出\n",
    "    test_accuracy.append(snet.accuracy(test, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossとaccuracyのグラフ化\n",
    "df_log = pd.DataFrame({\"train_loss\":train_loss,\n",
    "             \"test_loss\":test_loss,\n",
    "             \"train_accuracy\":train_accuracy,\n",
    "             \"test_accuracy\":test_accuracy})\n",
    "\n",
    "df_log.plot(style=['r-', 'r--', 'b-', 'b--'])\n",
    "plt.ylim([0,3])\n",
    "plt.ylabel(\"Accuracy or loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
