{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シンプルなニューラルネットワーククラスの実装\n",
    "* ここでは、シンプルなニューラルネットワーククラスを実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.activations import softmax\n",
    "from common.grad import numerical_gradient\n",
    "from common.loss import cross_entropy_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 「ニューラルネットワークにおいて勾配を求める」とは、損失$L$の$W$に対する偏微分$\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}}$を求めるということである。  \n",
    "    例、  \n",
    "    ${\\bf W}=\\begin{pmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{pmatrix}$  \n",
    "  \n",
    "    $\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}}=\\begin{pmatrix} \\frac{\\partial L}{\\partial {w_{11}}} &\n",
    "                                                                                      \\frac{\\partial L}{\\partial {w_{12}}} &\n",
    "                                                                                      \\frac{\\partial L}{\\partial {w_{13}}} \\\\\n",
    "                                                                                      \\frac{\\partial L}{\\partial {w_{21}}} &\n",
    "                                                                                      \\frac{\\partial L}{\\partial {w_{22}}} &\n",
    "                                                                                      \\frac{\\partial L}{\\partial {w_{23}}}\n",
    "                                                                                      \\end{pmatrix}$  \n",
    "                                                                                            \n",
    "  \n",
    "    \n",
    "* $\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}}$は、${\\bf W}$のそれぞれの要素が微小量変化したときの$L$の変化量を表す。\n",
    "\n",
    "\n",
    "### [演習]\n",
    "* シンプルなニューラルネットワークにおいて、予測、損失、勾配を計算する以下のクラスを完成させましょう\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.47143516 -1.19097569  1.43270697]\n",
      " [-0.3126519  -0.72058873  0.88716294]]\n"
     ]
    }
   ],
   "source": [
    "# ヒント\n",
    "np.random.seed(seed=1234)\n",
    "print(np.random.randn(2,3)) # 標準正規分布からランダムにサンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleNet:\n",
    "    def __init__(self, seed=1234):\n",
    "        np.random.seed(seed)\n",
    "        self.W = np.random.randn(2,3)#Wの初期値\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        予測関数\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W)\n",
    "        y = softmax(z)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        勾配計算関数\n",
    "        \"\"\"\n",
    "        grads={}\n",
    "        f = self.loss\n",
    "        W = self.W\n",
    "        grads[\"W\"] = numerical_gradient(f, x, W, t)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net.W=\n",
      "[[ 0.47143516 -1.19097569  1.43270697]\n",
      " [-0.3126519  -0.72058873  0.88716294]]\n",
      "\n",
      "y= [0.15391894 0.03932405 0.80675701]\n",
      "\n",
      "np.argmax(y)= 2\n",
      "\n",
      "loss= 0.21473263744557\n",
      "\n",
      "grad=\n",
      "{'W': array([[ 0.09235135,  0.02359443, -0.11594578],\n",
      "       [ 0.13852703,  0.03539164, -0.17391867]])}\n"
     ]
    }
   ],
   "source": [
    "# 重みの初期値の確認\n",
    "snet = simpleNet()        \n",
    "print(\"snet.W=\")\n",
    "print(snet.W)\n",
    "print()\n",
    "\n",
    "# 予測関数の確認\n",
    "x = np.array([0.6, 0.9])\n",
    "y = snet.predict(x)\n",
    "print(\"y=\", y)\n",
    "print()\n",
    "print(\"np.argmax(y)=\", np.argmax(y))\n",
    "print()\n",
    "\n",
    "# 損失を求める関数の確認\n",
    "t =np.array([0, 0, 1]) #正解ラベル\n",
    "print(\"loss=\",snet.loss(x, t))\n",
    "print()\n",
    "\n",
    "# 勾配を求める\n",
    "grad = snet.gradient(x, t)\n",
    "print(\"grad=\")\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [gradの解釈]\n",
    "* 求められたgradは、例えば、次のように解釈できる\n",
    "    * $w_{11}$を正の方向に微小量$h$だけ変化させたとき、$L$は約0.092増える。\n",
    "    * $w_{13}$を負の方向に微小量$h$だけ変化させたとき、$L$は約0.116増える。\n",
    "    * $L$は$0$に近づけたいので、$w_{11}$は負の方向に更新し、$w_{13}$は正の方向に更新するのが良い、となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* 重みWの初期値を変えてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
