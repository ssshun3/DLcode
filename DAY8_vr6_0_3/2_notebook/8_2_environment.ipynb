{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [迷路問題用] 環境を定義するためのEnvironmentクラスを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from common.meiro import State, Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \"\"\"\n",
    "    環境のクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, grid, move_prob=0.8, agent_init_state=(0,0)):\n",
    "        \"\"\"\n",
    "        grid : 2d-array, 迷路の各セルの条件\n",
    "                0: 通常のセル\n",
    "                -1: 落とし穴 (ゲーム終了)\n",
    "                1: ゴール (ゲーム終了)\n",
    "                9: 壁 (移動不可)\n",
    "        move_prob : float, 移動確率\n",
    "        \"\"\"\n",
    "        # 迷路条件を設定する\n",
    "        self.grid = grid\n",
    "        self.row_length = grid.shape[0]\n",
    "        self.col_length = grid.shape[1]\n",
    "        \n",
    "        # 行動を定義する\n",
    "        self.Action = Action()\n",
    "        self.actions = list(self.Action.dic_actions.keys())\n",
    "        \n",
    "        # 状態を定義する\n",
    "        self.states = self.get_all_states()\n",
    "        \n",
    "        # エージェントの位置を初期化する\n",
    "        self.reset(agent_init_state)\n",
    "            \n",
    "        # 報酬の初期値(通常セルに割り当てられる)\n",
    "        # 通常セルの報酬をマイナス値に設定しておくと、エージェントが早くゴールに到達するようになる\n",
    "        self.default_reward = -0.04\n",
    "\n",
    "        # エージェントが選択した方向に移動する確率\n",
    "        # エージェントは、(1- move_prob)の確率で、選択した方向とは異なる方向に移動する\n",
    "        # これは、実際の環境において制御通りに移動できないことを想定している\n",
    "        self.move_prob = move_prob\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\"\n",
    "        全ての状態を定義するためのメソッド\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for row in range(self.row_length):\n",
    "            for col in range(self.col_length):\n",
    "                # ブロックを配置した場所は状態を定義する必要がないのでスキップする\n",
    "                if self.grid[row][col] != 9:\n",
    "                    states.append(State(row, col))\n",
    "        return states\n",
    "\n",
    "    def reset(self, agent_init_state=(0,0)):\n",
    "        \"\"\"\n",
    "        エージェントの位置を初期化するメソッド\n",
    "        \"\"\"\n",
    "        # エージェントの初期位置を設定する\n",
    "        self.agent_state = State(*agent_init_state)\n",
    "        return self.agent_state    \n",
    "    \n",
    "    def check_next_state(self, state, action):\n",
    "        \"\"\"\n",
    "        次の状態を確認するためのメソッド\n",
    "        \"\"\"\n",
    "\n",
    "        # 現状の状態オブジェクトを複製する\n",
    "        next_state = state.clone()\n",
    "\n",
    "        # エージェントを移動させる\n",
    "        if action == 0: #\"UP\"\n",
    "            next_state.row -= 1\n",
    "        elif action == 1: #\"DOWN\"\n",
    "            next_state.row += 1\n",
    "        elif action == 2: #\"LEFT\"\n",
    "            next_state.col -= 1\n",
    "        elif action == 3: #\"RIGHT\"\n",
    "            next_state.col += 1\n",
    "\n",
    "        # 更新後の状態が迷路からはみ出してしまう場合は、更新しないことにする        \n",
    "        if not (0 <= next_state.row < self.row_length):\n",
    "            next_state = state\n",
    "        if not (0 <= next_state.col < self.col_length):\n",
    "            next_state = state\n",
    "\n",
    "        # 更新後の状態がブロックセルに入ってしまう場合は、更新しないことにする\n",
    "        if self.grid[next_state.row][next_state.col] == 9:\n",
    "            next_state = state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def calc_transit_prob(self, state, action):\n",
    "        \"\"\"\n",
    "        遷移確率を計算するメソッド\n",
    "        \"\"\"\n",
    "        dic_transition_probs = {}\n",
    "        if not self.can_action_at(state):\n",
    "            \"\"\"\n",
    "            ゴールもしくは、穴にいる場合\n",
    "            \"\"\"\n",
    "            return dic_transition_probs\n",
    "\n",
    "        for a in self.actions:\n",
    "            if a == action:\n",
    "                \"\"\"\n",
    "                選ばれた方向の場合\n",
    "                \"\"\"\n",
    "                prob = self.move_prob\n",
    "            else:\n",
    "                \"\"\"\n",
    "                選ばれた方向と異なる方向\n",
    "                \"\"\"\n",
    "                # 選ばれた方向以外に進む確率を3等分する\n",
    "                prob = (1 - self.move_prob) / 3\n",
    "            \n",
    "            # エージェントの稼働範囲に応じて移動確率を調整する\n",
    "            next_state = self.check_next_state(state, a) # あるaが移動できない行動である場合は、現状のstateがそのまま返ってくる\n",
    "            if next_state not in dic_transition_probs:\n",
    "                dic_transition_probs[next_state] = prob\n",
    "            else:\n",
    "                dic_transition_probs[next_state] += prob\n",
    "\n",
    "        return dic_transition_probs\n",
    "\n",
    "    def can_action_at(self, state):\n",
    "        \"\"\"\n",
    "        あるセル(state)が行動可能なセルかどうかを確認するメソッド\n",
    "        \"\"\"\n",
    "        if self.grid[state.row][state.col] == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def reward_func(self, state):\n",
    "        \"\"\"\n",
    "        報酬関数\n",
    "        \"\"\"\n",
    "        reward = self.default_reward # 報酬の初期値\n",
    "        done = False\n",
    "\n",
    "        # 今の状態の設定を確認する\n",
    "        attribute = self.grid[state.row][state.col]\n",
    "        \n",
    "        # 今の状態の設定に応じて、報酬を与える\n",
    "        if attribute == 1:\n",
    "            \"\"\"\n",
    "            ゴールに到達した場合\n",
    "            \"\"\"\n",
    "            reward = 1 # 報酬を与える\n",
    "            done = True # 終了フラグ\n",
    "        elif attribute == -1:\n",
    "            \"\"\"\n",
    "            穴に落ちた場合\n",
    "            \"\"\"\n",
    "            reward = -1 # 負の報酬を与える\n",
    "            done = True # 終了フラグ\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def transit(self, state, action):\n",
    "        \"\"\"\n",
    "        遷移関数\n",
    "        次の状態(移動先)を決定し、報酬を算出する\n",
    "        \"\"\"\n",
    "        \n",
    "        # 遷移確率を計算する\n",
    "        dic_transition_probs = self.calc_transit_prob(state, action)\n",
    "        if len(dic_transition_probs) == 0:\n",
    "            \"\"\"\n",
    "            ゴールもしくは穴にいる場合\n",
    "            \"\"\"\n",
    "            return None, None, True\n",
    "\n",
    "        # dic_transition_probsをリストに変換する\n",
    "        next_states = []\n",
    "        probs = []\n",
    "        for s, prob in dic_transition_probs.items():\n",
    "            next_states.append(s)\n",
    "            probs.append(prob)\n",
    "\n",
    "        # 次の状態(移動先)を確率に基づいて決定する\n",
    "        next_state = np.random.choice(next_states, p=probs)\n",
    "        \n",
    "        # 報酬を算出する\n",
    "        reward, done = self.reward_func(next_state)\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        ある行動を実際にとってみたときの、次の状態と獲得報酬額を決めるための関数\n",
    "        \"\"\"\n",
    "        # 今の状態からある行動をとってみると、次の状態と獲得報酬額がわかる\n",
    "        next_state, reward, done = self.transit(self.agent_state, action)\n",
    "        \n",
    "        # 状態を更新する\n",
    "        if next_state is not None:\n",
    "            self.agent_state = next_state\n",
    "\n",
    "        return next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 0\n",
      "\n",
      "エージェントの現在位置 : 0 , 0\n",
      "これからとる行動 : 0, UP\n",
      "エージェントの移動後の位置 :  0 , 0   報酬額 : -0.04\n",
      "\n",
      "エージェントの現在位置 : 0 , 0\n",
      "これからとる行動 : 1, DOWN\n",
      "エージェントの移動後の位置 :  1 , 0   報酬額 : -0.04\n",
      "\n",
      "エージェントの現在位置 : 0 , 0\n",
      "これからとる行動 : 2, LEFT\n",
      "エージェントの移動後の位置 :  0 , 0   報酬額 : -0.04\n",
      "\n",
      "エージェントの現在位置 : 0 , 0\n",
      "これからとる行動 : 3, RIGHT\n",
      "エージェントの移動後の位置 :  0 , 1   報酬額 : -0.04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 迷路の条件を設定する\n",
    "grid = np.array([\n",
    "                            [0, 0, 0, 1],\n",
    "                            [0, 9, 0, -1],\n",
    "                            [0, 0, 0, 0]\n",
    "                        ])\n",
    "\n",
    "# 環境をつくる\n",
    "env = Environment(grid, move_prob=0.95, agent_init_state=(0,0))\n",
    "\n",
    "# エージェントの初期位置\n",
    "print(env.agent_state.row, \",\" , env.agent_state.col)\n",
    "print()\n",
    "\n",
    "# ある行動をとったときの次の状態と獲得報酬額を求める\n",
    "for a in env.actions:\n",
    "    \n",
    "    # エージェントの位置の初期化\n",
    "    env.reset(agent_init_state=(0,0))\n",
    "    print(\"エージェントの現在位置 :\", env.agent_state.row, \",\", env.agent_state.col)\n",
    "    \n",
    "    # 行動の種類\n",
    "    print(\"これからとる行動 : %s, %s\"%(a, env.Action.dic_actions[a]))\n",
    "    \n",
    "    # 行動をとる\n",
    "    next_state, reward, done = env.step(action=a)\n",
    "    print(\"エージェントの移動後の位置 : \", next_state.row, \",\", next_state.col,  \"  報酬額 :\", reward)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* grid条件やエージェントの初期位置を変更して、エージェントを1セル移動させてみましょう"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
