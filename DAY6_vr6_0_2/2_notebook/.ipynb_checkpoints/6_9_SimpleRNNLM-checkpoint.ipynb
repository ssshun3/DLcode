{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleRNNLMクラスを実装する\n",
    "SimpleRNNLMクラスは、RNNレイヤを用いた言語モデルを構築するためのクラスである"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.time_layers import TimeEmbedding, TimeRNN, TimeAffine, TimeSoftmaxWithLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* 以下のSimpleRNNLMのクラスを完成させましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        \n",
    "        # 乱数生成関数を別名で定義する\n",
    "        rn = np.random.randn # 標準正規分布からサンプリングする関数\n",
    "\n",
    "        # パラメータの初期化\n",
    "        embed_W = rn(V, D) / 100 # 小さな値で初期化する\n",
    "        rnn_Wx = rn(D, H) * np.sqrt(2/(D+H)) # Xavierの初期値\n",
    "        rnn_Wh = rn(H, H) * np.sqrt(2/(H+H)) # Xavierの初期値\n",
    "        rnn_b = np.zeros(H)\n",
    "        affine_W = rn(H, V) * np.sqrt(2/(H+V)) # Xavierの初期値\n",
    "        affine_b = np.zeros(V)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W), # 単語埋め込みレイヤ\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True), # RNNレイヤ. 中間層の状態を引き継ぐため、statefulをTrueにしておく\n",
    "            TimeAffine(affine_W, affine_b) # Affineレイヤ\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss() # 損失レイヤ\n",
    "\n",
    "        # すべてのパラメータと勾配をそれぞれ1つのリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params # リストを結合する\n",
    "            self.grads += layer.grads # リストを結合する\n",
    "\n",
    "    def predict(self, xs):\n",
    "        \"\"\"\n",
    "        予測関数\n",
    "        \"\"\"\n",
    "        # 損失レイヤ以外の全てのレイヤについて計算する\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "    \n",
    "    def forward(self, xs, ts):\n",
    "        \"\"\"\n",
    "        順伝播計算\n",
    "        xs : 入力の単語ID, 配列形状は(ミニバッチ数、時間数)\n",
    "        \"\"\"\n",
    "        # 損失レイヤ以外の全てのレイヤについて計算する\n",
    "        xs = self.predict(xs)\n",
    "        \n",
    "        # 損失レイヤを計算する\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \"\"\"\n",
    "        逆伝播計算\n",
    "        \"\"\"\n",
    "        # 損失レイヤの逆伝播計算\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        \n",
    "        # 損失レイヤ以外の逆伝播計算\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 2.3030992264465704\n",
      "\n",
      "dout= None\n",
      "\n",
      "Embeddingレイヤの重み行列=\n",
      " [array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [-0.00564394,  0.02291211,  0.07414348],\n",
      "       [ 0.00991297,  0.03485853, -0.04722009],\n",
      "       [-0.01543512,  0.04187022,  0.01060033],\n",
      "       [ 0.04821005, -0.02210823, -0.02774173],\n",
      "       [-0.0133208 , -0.02983754,  0.00130878],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-0.04044225,  0.01156308, -0.03507822],\n",
      "       [-0.05571195,  0.02299591,  0.01151434],\n",
      "       [ 0.        ,  0.        ,  0.        ]])]\n"
     ]
    }
   ],
   "source": [
    "V = 10 # 語彙数\n",
    "D = 3 # 埋め込みベクトルのノード数\n",
    "H = 3 # RNNレイヤの隠れ層のノード数\n",
    "srnnlm = SimpleRnnlm(V, D, H)\n",
    "\n",
    "N = 2 # バッチサイズ\n",
    "T = 4 # 時間数\n",
    "\n",
    "# 単語ID\n",
    "xs = np.array([[2, 5, 1, 4],[3, 2, 7, 8]]) # 入力の単語ID\n",
    "ts = np.array([[5, 1, 4, 3],[2, 7, 8, 0]]) # 正解の単語ID\n",
    "\n",
    "# 順伝播計算\n",
    "loss = srnnlm.forward(xs, ts)\n",
    "print(\"loss=\", loss)\n",
    "print()\n",
    "\n",
    "# 逆伝播計算\n",
    "dout = srnnlm.backward()\n",
    "print(\"dout=\", dout) # 最後の層はEmbeddingレイヤなので、Noneが返ってくる\n",
    "print()\n",
    "\n",
    "# 勾配の確認\n",
    "print(\"Embeddingレイヤの重み行列=\\n\", srnnlm.layers[0].grads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
