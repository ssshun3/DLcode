{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確率的勾配降下法とその派生系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import * #コンタ描画用\n",
    "# import myplotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* 以下の各最適化手法のクラスを完成させましょう  \n",
    "    SGD  \n",
    "    Momentum  \n",
    "    NesterovAG  \n",
    "    Adagrad  \n",
    "    RMSProp  \n",
    "    Adadelta  \n",
    "    Adam  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 確率的勾配降下法(Stochastic Gradient Descent)\n",
    "* SGDの更新式  \n",
    "  \n",
    "    $\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$  \n",
    "      \n",
    "    ${\\boldsymbol \\theta}$ : 重み  \n",
    "    $L$ : 損失  \n",
    "    $\\eta$ : 学習率  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        \"\"\"\n",
    "        lr : 学習係数 learning rate\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        重みの更新\n",
    "        \"\"\"\n",
    "        for key in params.keys():\n",
    "            params[key] -=                                      # <- 穴埋め"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "* Momentumの更新式  \n",
    "  \n",
    "    $\\displaystyle {\\boldsymbol v}_{t+1} = \\alpha {\\boldsymbol v}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$  \n",
    "    $\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t + {\\boldsymbol v}_{t+1}$  \n",
    "      \n",
    "    $\\boldsymbol \\theta$ : 重み    \n",
    "    $L$ : 損失  \n",
    "    $\\eta$ : 学習率  \n",
    "    $\\alpha$ : モーメンタム係数(0以上1未満)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    \"\"\"\n",
    "    Momentum\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        \"\"\"\n",
    "        lr : 学習係数 learning rate\n",
    "        momentm : モーメンタム係数\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        重みの更新\n",
    "        \"\"\"\n",
    "        if self.v is None:\n",
    "            \"\"\"\n",
    "            初回のみ\n",
    "            \"\"\"\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] =                                   # <- 穴埋め\n",
    "            params[key] += self.v[key]\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Nesterov Accelerated Gradient\n",
    "* Nesterov Accelerated Gradientの更新式(定義)  \n",
    "  \n",
    "    $\\displaystyle {\\boldsymbol v}_{t+1} = \\alpha {\\boldsymbol v}_t - \\eta \\frac{\\partial L}{\\partial ({\\boldsymbol \\theta}_t+\\alpha {\\boldsymbol v}_t)}$  \n",
    "    $\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t + {\\boldsymbol v}_{t+1}$  \n",
    "      \n",
    "    $\\boldsymbol \\theta$ : 重み  \n",
    "    $L$ : 損失  \n",
    "    $\\eta$ : 学習率  \n",
    "    $\\alpha$ : モーメンタム係数(0以上1未満)\n",
    "      \n",
    "        \n",
    "    * Momentumの改良版。次の仮の位置を求め、その位置の勾配を利用する。\n",
    "    * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "    * https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf\n",
    "    * https://arxiv.org/pdf/1212.0901v2.pdf\n",
    "    \n",
    "* Nesterov Accelerated Gradientの更新式(実装) \n",
    "  \n",
    "    $\\displaystyle {\\boldsymbol \\Theta}_{t+1} = {\\boldsymbol \\theta}_{t+1} + \\alpha {\\boldsymbol v}_{t+1}$とおくと、  \n",
    "    $\\displaystyle {\\boldsymbol \\Theta}_{t} = {\\boldsymbol \\theta}_{t} + \\alpha {\\boldsymbol v}_{t}$ なので、定義式は以下のように変形できる\n",
    "    \n",
    "    $\\displaystyle {\\boldsymbol v}_{t+1} = \\alpha {\\boldsymbol v}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol \\Theta}_t}$  \n",
    "      \n",
    "    $\\displaystyle {\\boldsymbol \\Theta}_{t+1} = {\\boldsymbol  \\Theta}_{t} - \\alpha {\\boldsymbol v}_t + \\alpha {\\boldsymbol v}_{t+1} + {\\boldsymbol v}_{t+1}$  \n",
    "    $\\displaystyle  ~~~~~~~  = {\\boldsymbol \\Theta}_{t} - \\alpha {\\boldsymbol v}_t + (\\alpha+1) {\\boldsymbol v}_{t+1}$    \n",
    "    $\\displaystyle  ~~~~~~~  = {\\boldsymbol \\Theta}_{t} - \\alpha {\\boldsymbol v}_t + (\\alpha+1)  \\biggl( \\alpha {\\boldsymbol v}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol  \\Theta}_t} \\biggr)$        \n",
    "    $\\displaystyle  ~~~~~~~ = {\\boldsymbol  \\Theta}_{t} + \\alpha\\alpha {\\boldsymbol v}_t - (1+\\alpha) \\eta \\frac{\\partial L}{\\partial {\\bf \\Theta}_t} $  \n",
    "    $\\displaystyle  ~~~~~~~ = {\\boldsymbol  \\Theta}_{t} + \\alpha \\biggl(\\alpha {\\boldsymbol  v}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol  \\Theta}_t} \\biggr) - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol  \\Theta}_t}$      \n",
    "      \n",
    "    $\\boldsymbol \\theta$ : 重み  \n",
    "    $L$ : 損失  \n",
    "    $\\eta$ : 学習率  \n",
    "    $\\alpha$ : モーメンタム係数(0以上1未満)\n",
    "    \n",
    "[chainerでの実装] :   https://github.com/chainer/chainer/blob/master/chainer/optimizers/nesterov_ag.py  \n",
    "[kearaでの実装] :   \n",
    "https://github.com/keras-team/keras/blob/master/keras/optimizers.py#L74  \n",
    "[theanoでの実装] :   https://gist.github.com/kastnerkyle/816134462577399ee8b2  \n",
    "[nnablaでの実装] :   https://github.com/sony/nnabla/blob/master/src/nbla/solver/generic/nesterov.cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NesterovAG:\n",
    "    \"\"\"\n",
    "    Nesterov Accelerated Gradient\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        \"\"\"\n",
    "        lr : 学習係数 learning rate\n",
    "        momentm : モーメンタム係数\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        重みの更新\n",
    "        \"\"\"\n",
    "        if self.v is None:\n",
    "            \"\"\"\n",
    "            初回のみ\n",
    "            \"\"\"\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        # 重みを更新\n",
    "        for key in params.keys():\n",
    "            v_pre =                                                   # <- 穴埋め\n",
    "            self.v[key] =                                             # <- 穴埋め\n",
    "            params[key] +=                                            # <- 穴埋め"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ヒント\n",
    "# アダマール積\n",
    "a = np.array([[1,2],[3,4]])\n",
    "print(a, \"\\n\")\n",
    "print(a*a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "* AdaGradの更新式  \n",
    "  \n",
    "    $\\displaystyle {\\boldsymbol  h}_{t+1} = {\\boldsymbol  h}_{t} + \\frac{\\partial L}{\\partial {\\boldsymbol  \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol  \\theta}_t}$  \n",
    "    $\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t - \\eta \\frac{1}{ \\epsilon + \\sqrt{{\\boldsymbol  h}_{t+1}}} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol  \\theta}_t}$  \n",
    "      \n",
    "    $\\boldsymbol  \\theta$ : 重み    \n",
    "    $L$ : 損失  \n",
    "    $\\eta$ : 学習率  \n",
    "    $\\epsilon$ : 計算を安定化させるための定数. Goodfellow著の深層学習では1e-7    \n",
    "    $\\odot$ : アダマール積記号(同じ要素同士の掛け算)  \n",
    "      \n",
    "      \n",
    "    * hは必ず正の値になる\n",
    "    * 計算が進むにつれてhは大きくなる =これまで経験した勾配の値を2乗和として保持  \n",
    "    * hが大きくなると、みかけの学習率が小さくなり、θ の更新が遅くなる(=>学習が進まなくなることも)\n",
    "    * 重みごとに更新速度が変わるところがポイント\n",
    "    * これまでの勾配の積算量が大きくなると、更新速度が遅くなっていく\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad:\n",
    "    \"\"\"\n",
    "    Adagrad\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        \"\"\"\n",
    "        lr : 学習係数 learning rate\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        self.epsilon = 1e-7\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        重みの更新\n",
    "        \"\"\"\n",
    "        if self.h is None:\n",
    "            \"\"\"\n",
    "            初回のみ\n",
    "            \"\"\"\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] +=                                                 # <- 穴埋め\n",
    "            params[key] -=                                                 # <- 穴埋め"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaDelta\n",
    "* AdaDeltaの更新式  \n",
    "\n",
    "    $\\displaystyle {\\boldsymbol g}_t = \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$\n",
    "      \n",
    "     勾配の2乗の移動平均を求める  \n",
    "    $\\displaystyle {\\boldsymbol h}_{t+1} = \\rho{\\boldsymbol h}_{t} + (1-\\rho){\\boldsymbol g}_t \\odot {\\boldsymbol g}_t$   \n",
    "       \n",
    "     1ステップ前における更新量の2乗の移動平均のルートを求める  \n",
    "     ${\\boldsymbol rms\\_param} = \\sqrt{{\\boldsymbol r}_{t} + \\epsilon}$  \n",
    "         \n",
    "     勾配の2乗の移動平均のルートを求める  \n",
    "     ${\\boldsymbol rms\\_grad} = \\sqrt{{\\boldsymbol h}_{t+1} + \\epsilon}$\n",
    "       \n",
    "    更新量の算出  \n",
    "    $\\displaystyle \\Delta {\\boldsymbol \\theta}_t = - \\frac{{\\boldsymbol rms\\_param}}{{\\boldsymbol rms\\_grad}}\\odot {\\boldsymbol g}_t$\n",
    "       \n",
    "    重みの更新  \n",
    "    $\\displaystyle {\\boldsymbol \\theta}_{t+1} =  {\\boldsymbol \\theta}_t + \\Delta {\\boldsymbol \\theta}_t$  \n",
    "    \n",
    "    次ステップのために、更新量の2乗の移動平均を求める  \n",
    "    $\\displaystyle {\\boldsymbol r}_{t+1} = \\rho{\\boldsymbol r}_{t} + (1-\\rho) \\Delta {\\boldsymbol \\theta}_t  \\odot \\Delta {\\boldsymbol \\theta}_{t}$  \n",
    "     \n",
    "    $\\boldsymbol \\theta$ : 重み  \n",
    "    $L$ : 損失  \n",
    "    $\\epsilon$ : 計算を安定化させるための定数. 原著論文では1e-6  \n",
    "    $\\rho$ : 減衰率. 0.95など   \n",
    "    $\\odot$ : アダマール積記号(同じ要素同士の掛け算)    \n",
    "    * $\\eta$が$rms\\_param$におきかわったことで、重みの単位と更新量の単位が一致することになる。        \n",
    "    * 勾配の2乗の移動平均と、更新量の移動平均を用いて、見かけの学習率を変化させていく。\n",
    "    * 移動平均をとると、過去の情報が少しずつ薄れていき、新しい情報の影響がより大きくなっていく。\n",
    "    * https://arxiv.org/abs/1212.5701\n",
    "    * AdaDeltaの簡易版がRMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adadelta:\n",
    "    \"\"\"\n",
    "    Adadelta\n",
    "    \"\"\"\n",
    "    def __init__(self, rho=0.95):\n",
    "        \"\"\"\n",
    "        rho : 減衰率\n",
    "        \"\"\"\n",
    "        self.h = None\n",
    "        self.r = None        \n",
    "        self.rho = rho\n",
    "        self.epsilon = 1e-6\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        重みの更新\n",
    "        \"\"\"\n",
    "        if self.h is None:\n",
    "            \"\"\"\n",
    "            初回のみ\n",
    "            \"\"\"\n",
    "            self.h = {}\n",
    "            self.r = {}            \n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                self.r[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            \n",
    "            # 1ステップ前における更新量の2乗の移動平均のルートを求める\n",
    "            rms_param =                                                       # <- 穴埋め\n",
    "                        \n",
    "            # 勾配の2乗の移動平均を求める\n",
    "            self.h[key] =                                                     # <- 穴埋め\n",
    "            \n",
    "            # 勾配の2乗の移動平均のルートを求める\n",
    "            rms_grad =                                                        # <- 穴埋め\n",
    "            \n",
    "            # 更新量の算出\n",
    "            dp =                                                              # <- 穴埋め\n",
    "            \n",
    "            # 重みの更新\n",
    "            params[key] += dp\n",
    "            \n",
    "            # 次ステップのために、更新量の2乗の移動平均を求める \n",
    "            self.r[key]  =                                                    # <- 穴埋め"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "* RMSPropの更新式  \n",
    "  \n",
    "    $\\displaystyle {\\boldsymbol h}_{t+1} = \\rho{\\boldsymbol h}_{t}+ (1-\\rho)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$  \n",
    "    $\\displaystyle {\\boldsymbol \\theta}_{t+1} =  {\\boldsymbol \\theta}_t - \\eta \\frac{1}{\\sqrt{{\\boldsymbol h}_{t+1}+\\epsilon}} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$  \n",
    "      \n",
    "    $\\boldsymbol \\theta$ : 重み    \n",
    "    $L$ : 損失  \n",
    "    $\\eta$ : 学習率  \n",
    "    $\\epsilon$ : 計算を安定化させるための定数. Goodfellow著の深層学習では1e-6  \n",
    "    $\\rho$ : 減衰率. 0.9など   \n",
    "    $\\odot$ : アダマール積記号(同じ要素同士の掛け算)  \n",
    "      \n",
    "    \n",
    "        \n",
    "    * 勾配の2乗の移動平均を用いて、見かけの学習率を変化させていく。\n",
    "    * 移動平均をとると、過去の情報が少しずつ薄れていき、新しい情報が反映されていく。\n",
    "    * AdaDeltaの簡易版。\n",
    "    * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    \"\"\"\n",
    "    RMSProp\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, rho=0.9):\n",
    "        \"\"\"\n",
    "        lr : 学習係数 learning rate\n",
    "        rho : 減衰率\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        self.rho = rho\n",
    "        self.epsilon = 1e-6\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        重みの更新\n",
    "        \"\"\"\n",
    "        if self.h is None:\n",
    "            \"\"\"\n",
    "            初回のみ\n",
    "            \"\"\"\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key] =                                                     # <- 穴埋め\n",
    "            params[key] -=                                                    # <- 穴埋め"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "* Adamの更新式  \n",
    "    \n",
    "    $\\displaystyle {\\boldsymbol m}_{t+1} = \\rho_1 {\\boldsymbol m}_{t} + (1 - \\rho_1)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$  \n",
    "    $\\displaystyle {\\boldsymbol v}_{t+1} = \\rho_2 {\\boldsymbol v}_{t} + (1 - \\rho_2)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$\n",
    "    \n",
    "    $\\displaystyle {\\hat{\\boldsymbol m}}_{t+1} = \\frac{{\\boldsymbol m}_{t+1}}{1-\\rho_1^t}~~~~$   バイアスを修正  \n",
    "    $\\displaystyle {\\hat{\\boldsymbol v}}_{t+1} = \\frac{{\\boldsymbol v}_{t+1}}{1-\\rho_2^t}~~~~$     バイアスを修正\n",
    "    \n",
    "    $\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t - \\eta \\frac{1}{\\sqrt{\\hat{\\boldsymbol v}_{t+1}}+\\epsilon} \\odot {\\hat{\\boldsymbol m}}_{t+1}$  \n",
    "    \n",
    "    $\\boldsymbol \\theta$ : 重み    \n",
    "    $L$ : 損失  \n",
    "    $t$ : ステップ数  \n",
    "    $\\eta$ : 学習率  \n",
    "    $\\epsilon$ : 計算を安定化させるための定数. 原著論文では1e-8  \n",
    "    $\\rho_1$ : 減衰率. 0.9が推奨されている   \n",
    "    $\\rho_2$ : 減衰率. 0.999が推奨されている      \n",
    "    $\\odot$ : アダマール積記号(同じ要素同士の掛け算)  \n",
    "      \n",
    "        \n",
    "    * 勾配の2乗の移動平均(2次モーメント)だけでなく、勾配の移動平均(1次モーメント)も保持する\n",
    "    * RMSRPropとMomemtumを組み合わせような手法\n",
    "    * バイアス修正式は、学習初期の計算を安定させることが目的\n",
    "    * https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.001, rho1=0.9, rho2=0.999):\n",
    "        self.lr = lr\n",
    "        self.rho1 = rho1\n",
    "        self.rho2 = rho2\n",
    "        self.iter = 0\n",
    "        self.m = None # 1次モーメント. 勾配の平均に相当する\n",
    "        self.v = None  # 2次モーメント. 勾配の分散に相当する(中心化されていない)\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.m[key] =    # 1次モーメント                                                          # <- 穴埋め\n",
    "            self.v[key] =     #2次モーメント                                                          # <- 穴埋め\n",
    "            \n",
    "            # モーメントのバイアス補正\n",
    "            # 計算初期の頃のモーメントを補正することが目的\n",
    "            # 計算が進行する(self.iterが大きくなる)と、分母は1に近づく\n",
    "            m =   # 1次モーメント                                                                                # <- 穴埋め\n",
    "            v =    # 2次モーメント                                                                                # <- 穴埋め\n",
    "            \n",
    "            # 重みの更新\n",
    "            params[key] -=                                                    # <- 穴埋め"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完成したクラスをテストする. \n",
    "ここでは、ミニバッチ学習を行わないので、確率的勾配降下法ではなく勾配降下法で解いていることに注意\n",
    "  \n",
    "### 2次2変数の最小値探索問題(凸2次計画問題)\n",
    "関数 $f(x,y)=(x-3)^2+(2y-1)^2$の最小点を求めよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $f(x,y)=(x-3)^2+(2y-1)^2$のグラフを2次元等高線で描画する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxy(x, y):\n",
    "    return (x-3)**2 + (2*y-1)**2\n",
    "\n",
    "#目的関数の値をつくる\n",
    "axis_x = np.arange(-2,5,0.1)\n",
    "axis_y = np.arange(-2,2,0.1)\n",
    "mx, my = meshgrid(axis_x, axis_y) \n",
    "mz = fxy(mx, my)\n",
    "    \n",
    "#目的関数\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.pcolor(mx,my,mz,cmap=\"jet\")\n",
    "colb = plt.colorbar()\n",
    "colb.set_label(\"f(x,y)\")\n",
    "\n",
    "#目的関数の最小値\n",
    "plt.scatter(x=3,y=0.5,marker=\"x\",s=100,c=\"k\")\n",
    "\n",
    "#表示範囲\n",
    "plt.xlim([-2,5])\n",
    "plt.ylim([-2,2])\n",
    "\n",
    "#描画\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df):\n",
    "    \"\"\"\n",
    "    結果の描画\n",
    "    \"\"\"\n",
    "    #目的関数\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.pcolor(mx,my,mz,cmap=\"jet\")\n",
    "    colb = plt.colorbar()\n",
    "    colb.set_label(\"f(x,y)\")\n",
    "\n",
    "    #目的関数の解析的最小値\n",
    "    plt.scatter(x=3,y=0.5,marker=\"x\",s=300,c=\"black\")\n",
    "\n",
    "    #表示範囲\n",
    "    plt.xlim([-2,5])\n",
    "    plt.ylim([-2,2])\n",
    "\n",
    "    #探索点\n",
    "    plt.plot(df[\"x\"],df[\"y\"],marker=\"o\",c=\"k\")\n",
    "\n",
    "    #描画\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "\n",
    "def gradient(x, y):\n",
    "    \"\"\"\n",
    "    勾配を求める関数\n",
    "    \"\"\"\n",
    "    dx = 2 * x - 6.0\n",
    "    dy = 8.0 * y - 4.0\n",
    "    return {\"x\":dx, \"y\":dy}\n",
    "\n",
    "def func(optimizer, x_init=-1.9, y_init=-1.9, maxIter=10000, threshold=1.0e-8):\n",
    "    \"\"\"\n",
    "    x_init : x初期値\n",
    "    y_init : y初期値\n",
    "    maxIter : 最大ループ回数\n",
    "    threshold : 収束判定閾値\n",
    "    \"\"\"\n",
    "        \n",
    "    # 初期値の設定\n",
    "    residual = 1 \n",
    "    x = np.array([x_init])\n",
    "    y = np.array([y_init])\n",
    "    x_pre = x_init\n",
    "    y_pre = y_init\n",
    "    \n",
    "    #結果を格納するdataframe\n",
    "    i = 0 \n",
    "    df_re = pd.DataFrame()\n",
    "    df_re.loc[i,\"x\"] = x\n",
    "    df_re.loc[i,\"y\"] = y\n",
    "    df_re.loc[i,\"residual\"] = residual\n",
    "    \n",
    "    #計算\n",
    "    i += 1\n",
    "    while residual > threshold:\n",
    "        \n",
    "        params = {\"x\":x, \"y\":y}\n",
    "        # 勾配の更新\n",
    "        grads = gradient(x, y)         \n",
    "        # 座標の更新\n",
    "        optimizer.update(params, grads)\n",
    "        \n",
    "        x = params[\"x\"]\n",
    "        y = params[\"y\"]\n",
    "        \n",
    "        residual = (x - x_pre)**2 + (y - y_pre)**2\n",
    "        df_re.loc[i,\"x\"] = x\n",
    "        df_re.loc[i,\"y\"] = y\n",
    "        df_re.loc[i,\"residual\"] = residual\n",
    "        i += 1\n",
    "\n",
    "        x_pre = x.copy()\n",
    "        y_pre = y.copy()\n",
    "        \n",
    "        if i > maxIter:\n",
    "            break\n",
    "\n",
    "    print(i, np.round(x,3), np.round(y,3), residual)\n",
    "    return df_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"SGD\")\n",
    "optimizer = SGD(lr=0.2)\n",
    "df_re = func(optimizer)\n",
    "plot(df_re)\n",
    "\n",
    "print(\"Momentum\")\n",
    "optimizer = Momentum(lr=0.01, momentum=0.75)\n",
    "df_re = func(optimizer)\n",
    "plot(df_re)\n",
    "\n",
    "print(\"Nesterov AG\")\n",
    "optimizer = NesterovAG(lr=0.01, momentum=0.75)\n",
    "df_re = func(optimizer)\n",
    "plot(df_re)\n",
    "\n",
    "print(\"Adagrad\")\n",
    "optimizer = Adagrad(lr=0.1)\n",
    "df_re = func(optimizer)\n",
    "plot(df_re)\n",
    "\n",
    "print(\"RMSProp\")\n",
    "optimizer = RMSProp(lr=0.1,rho=0.9)\n",
    "df_re = func(optimizer)\n",
    "plot(df_re)\n",
    "\n",
    "print(\"Adadelta\")\n",
    "optimizer = Adadelta(rho=0.95)\n",
    "df_re = func(optimizer)\n",
    "plot(df_re)\n",
    "\n",
    "print(\"Adam\")\n",
    "optimizer = Adam(lr=0.1)\n",
    "df_re = func(optimizer)\n",
    "plot(df_re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* 学習率やモーメンタム係数を変更して、結果がどのように変わるか確認しましょう。\n",
    "* 手法、学習率、係数をいろいろ変化させて、収束までのステップ回数を比較してみましょう。一覧表をつくると比較しやすいでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
