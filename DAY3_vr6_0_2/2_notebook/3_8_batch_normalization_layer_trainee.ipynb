{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# バッチ正規化(batch normalization)レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バッチ正規化(batch normalization)レイヤの計算手順\n",
    "ここでの説明は、バッチ正規化レイヤの入力層側に設置されている層の出力値のうちの1ノード分を対象にする。  \n",
    "\n",
    "### [計算手順(学習時の順伝播計算)] \n",
    "#### (1) 計算の対象をxとする  \n",
    "$~~~$入力 :  ${\\bf x} = \\{x_1,x_2, \\dots , x_n\\}$  \n",
    "$~~~$n : データ数=バッチサイズ  \n",
    "  \n",
    "  \n",
    "#### (2) 入力の平均値を求める    \n",
    "$~~~$$\\displaystyle \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i$\n",
    "  \n",
    "  \n",
    "#### (3) 入力の分散を求める  \n",
    "$~~~$$\\displaystyle \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)^2$\n",
    "  \n",
    "  \n",
    "#### (4) 入力を標準化する  \n",
    "$~~~$各入力値について以下の処理を行う。numpyで計算する場合はベクトルとスカラーの演算が可能。  \n",
    "$~~~$$\\displaystyle \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2+\\epsilon}} $   \n",
    "$~~~$$\\epsilon$ : $1e-8$ (深層学習, Goodfellow, p.229)\n",
    "      \n",
    "      \n",
    "#### (5) スケールし、平行移動させる  \n",
    "$~~~$各入力値について以下の処理を行う。numpyで計算する場合はベクトルとスカラーの演算が可能。  \n",
    "$~~~$$\\displaystyle y_i = \\gamma \\hat{x}_i + \\beta $  \n",
    "$~~~$$y_i$が返り値になる。  \n",
    "$~~~$$\\gamma$と$\\beta$は、標準化された$x$の分布を最適な分布に変換するための係数であり、学習の過程で最適化されていくパラメータ。1つのミニバッチ内で計算される平均$\\mu$と分散$\\sigma^2$とは値が異なる。\n",
    "\n",
    "\n",
    "### [計算手順(予測時の順伝播計算)] \n",
    "基本的には、学習時の順伝播計算と同じだが、$\\mu$と$\\sigma^2$は、学習時に求めた移動平均値を使う\n",
    "$~~~$  \n",
    "$~~~$  \n",
    "\n",
    "  \n",
    "### [計算手順(学習時の逆伝播計算)] \n",
    "スライドの計算グラフを参照\n",
    "$~~~$  \n",
    "$~~~$  \n",
    "  \n",
    "  \n",
    "[参考]\n",
    "* 原著論文\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf\n",
    "* ブログ\n",
    "    * https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [演習]\n",
    "* 以下のバッチ正規化(batch normalization)レイヤクラスを完成させましょう.\n",
    "* 入力xは、バッチ正規化レイヤの入力層側に設置されている層の出力値. n*d行列になっていることに注意.  \n",
    "\n",
    "  入力 :  ${\\bf x}=\\quad\n",
    "    \\begin{pmatrix} \n",
    "    x_{11} & x_{12} & \\dots & x_{1d}\\\\\n",
    "    x_{21} & x_{22} & \\dots & x_{2d}\\\\\n",
    "   \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & \\dots & x_{nd}\\\\\n",
    "    \\end{pmatrix}\n",
    "    \\quad$\n",
    "\n",
    "    * ${\\bf x}$ は、n*d行列\n",
    "    * n : バッチサイズ  \n",
    "    * d : 入力層側の層のノード数    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ヒント\n",
    "x = np.array([[1,2,3],[2,3,2],[3,1,4],[4,1,2]]) # N×D行列\n",
    "print(\"x=\",x)\n",
    "mu = np.mean(x, axis=0) # 要素数D個のベクトル\n",
    "print(\"mu=\", mu)\n",
    "var = np.mean((x-mu)**2, axis=0)  # 要素数D個のベクトル\n",
    "print(\"var=\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, gamma, beta, rho=0.9, moving_mean=None, moving_var=None):\n",
    "        self.gamma = gamma # スケールさせるためのパラメータ, 学習によって更新させる.\n",
    "        self.beta = beta # シフトさせるためのパラメータ, 学習によって更新させる\n",
    "        self.rho = rho # 移動平均を算出する際に使用する係数\n",
    "\n",
    "        # 予測時に使用する平均と分散\n",
    "        self.moving_mean = moving_mean   # muの移動平均\n",
    "        self.moving_var = moving_var     # varの移動平均\n",
    "        \n",
    "        # 計算中に算出される値を保持しておく変数群\n",
    "        self.batch_size = None\n",
    "        self.x_mu = None\n",
    "        self.x_std = None        \n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        \"\"\"\n",
    "        順伝播計算\n",
    "        x :  CNNの場合は4次元、全結合層の場合は2次元  \n",
    "        \"\"\"\n",
    "        if x.ndim == 4:\n",
    "            \"\"\"\n",
    "            画像形式の場合\n",
    "            \"\"\"\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.transpose(0, 2, 3, 1) # NHWCに入れ替え\n",
    "            x = x.reshape(N*H*W, C) # (N*H*W,C)の2次元配列に変換\n",
    "            out = self.__forward(x, train_flg)\n",
    "            out = out.reshape(N, H, W, C)# 4次元配列に変換\n",
    "            out = out.transpose(0, 3, 1, 2) # 軸をNCHWに入れ替え\n",
    "        elif x.ndim == 2:\n",
    "            \"\"\"\n",
    "            画像形式以外の場合\n",
    "            \"\"\"\n",
    "            out = self.__forward(x, train_flg)           \n",
    "            \n",
    "        return out\n",
    "            \n",
    "    def __forward(self, x, train_flg, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        x : 入力. N×Dの行列. Nはバッチサイズ. Dは手前の層のノード数\n",
    "        \"\"\"\n",
    "        if (self.moving_mean is None) or (self.moving_var is None):\n",
    "            N, D = x.shape\n",
    "            self.moving_mean = np.zeros(D)\n",
    "            self.moving_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            \"\"\"\n",
    "            学習時\n",
    "            \"\"\"\n",
    "            # 入力xについて、Nの方向に平均値を算出. \n",
    "            mu =                                      # 要素数D個のベクトル                                                          # <- 穴埋め\n",
    "            mu = np.broadcast_to(mu, (N, D)) # Nの方向にブロードキャスト\n",
    "            print(\"mu.shape=\", mu.shape)\n",
    "            \n",
    "            # 入力xから平均値を引く\n",
    "            x_mu =                                 # N×D行列                                                                           # <- 穴埋め\n",
    "            print(\"x_mu.shape=\", x_mu.shape)\n",
    "            \n",
    "            # 入力xの分散を求める\n",
    "            var =                               # 要素数D個のベクトル                                                               # <- 穴埋め                         \n",
    "            print(\"var.shape=\", var.shape)\n",
    "            \n",
    "            # 入力xの標準偏差を求める(epsilonを足してから標準偏差を求める)\n",
    "            std =                           # 要素数D個のベクトル                                                                  # <- 穴埋め\n",
    "            print(\"std.shape=\", std.shape)\n",
    "            \n",
    "            # 標準偏差の逆数を求める\n",
    "            std_inv =                                                                                                                           # <- 穴埋め\n",
    "            std_inv = np.broadcast_to(std_inv, (N, D)) # Nの方向にブロードキャスト\n",
    "            print(\"std_inv.shape=\", std_inv.shape)\n",
    "            \n",
    "            # 標準化\n",
    "            x_std =                                 #N*D行列                                                                           # <- 穴埋め\n",
    "            print(\"x_std.shape=\", x_std.shape)\n",
    "            \n",
    "            # 値を保持しておく\n",
    "            self.batch_size = x.shape[0]\n",
    "            self.x_mu = x_mu\n",
    "            self.x_std = x_std\n",
    "            self.std = std\n",
    "            self.moving_mean = self.rho * self.moving_mean + (1-self.rho) *                    # <- 穴埋め\n",
    "            self.moving_var = self.rho * self.moving_var + (1-self.rho) *                            # <- 穴埋め      \n",
    "        else:\n",
    "            \"\"\"\n",
    "            予測時\n",
    "            \"\"\"\n",
    "            x_mu =                   # N×D行列                                                                                  # <- 穴埋め\n",
    "            x_std =                  # N×D行列                                                                                  # <- 穴埋め\n",
    "            \n",
    "        # gammaでスケールし、betaでシフトさせる\n",
    "        out =                         # N×D行列                                                                                 # <- 穴埋め\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        逆伝播計算\n",
    "        dout : CNNの場合は4次元、全結合層の場合は2次元  \n",
    "        \"\"\"\n",
    "        if dout.ndim == 4:\n",
    "            \"\"\"\n",
    "            画像形式の場合\n",
    "            \"\"\"            \n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.transpose(0, 2, 3, 1) # NHWCに入れ替え\n",
    "            dout = dout.reshape(N*H*W, C) # (N*H*W,C)の2次元配列に変換\n",
    "            dx = self.__backward(dout)\n",
    "            dx = dx.reshape(N, H, W, C)# 4次元配列に変換\n",
    "            dx = dx.transpose(0, 3, 1, 2) # 軸をNCHWに入れ替え\n",
    "        elif dout.ndim == 2:\n",
    "            \"\"\"\n",
    "            画像形式以外の場合\n",
    "            \"\"\"\n",
    "            dx = self.__backward(dout)\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        \"\"\"\n",
    "        ここを完成させるには、計算グラフを理解する必要があり、実装にかなり時間がかかる.\n",
    "        \"\"\"\n",
    "        N, D = self.x_mu.shape\n",
    "        \n",
    "        # betaの勾配\n",
    "        dbeta =                                                                                                 # <- 穴埋め\n",
    "        \n",
    "        # gammaの勾配(Nの方向に合計)\n",
    "        dgamma =                                                                                            # <- 穴埋め\n",
    "        \n",
    "        # Xstdの勾配\n",
    "        a1 =                                                                                                     # <- 穴埋め\n",
    "        print(\"a1.shape=\", a1.shape)\n",
    "        \n",
    "        # Xmuの勾配(1つ目)\n",
    "        a2 =                                                                                                   # <- 穴埋め\n",
    "        print(\"a2.shape=\", a2.shape)\n",
    "        \n",
    "        # 標準偏差の逆数の勾配\n",
    "        a3 =                                                                                                  # <- 穴埋め\n",
    "        print(\"a3.shape=\", a3.shape)\n",
    "        a3 = np.sum(a3, axis=0) # Nの方向に合計\n",
    "        \n",
    "        # 標準偏差の勾配\n",
    "        a4 =                                                                                                    # <- 穴埋め\n",
    "        print(\"a4.shape=\", a4.shape)\n",
    "        \n",
    "        # 分散の勾配\n",
    "        a5 =                                                                                                      # <- 穴埋め\n",
    "        print(\"a5.shape=\", a5.shape)\n",
    "        \n",
    "        # Xmuの2乗の勾配\n",
    "        a6 =                                                                                                   # <- 穴埋め\n",
    "        a6 = np.broadcast_to(a6, (N, D)) # Nの方向にブロードキャスト\n",
    "        print(\"a6=\",a6)\n",
    "        print(\"a6.shape=\", a6.shape)\n",
    "        \n",
    "        # Xmuの勾配(2つ目)\n",
    "        a7 =                                                                                                   # <- 穴埋め\n",
    "        print(\"a7.shape=\", a7.shape)\n",
    "        \n",
    "        # muの勾配\n",
    "        a8 =                                                                                                   # <- 穴埋め\n",
    "        print(\"a8.shape=\", a8.shape)\n",
    "        a8 = np.sum(a8, axis=0) # Nの方向に合計\n",
    "\n",
    "        # Xの勾配\n",
    "        a9 =                                                                                                   # <- 穴埋め\n",
    "        a9 = np.broadcast_to(a9, (N, D)) # Nの方向にブロードキャスト\n",
    "        dx =                                                                                                    # <- 穴埋め\n",
    "        print(\"a9.shape=\", a9.shape)\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力が2次元の場合\n",
    "hidden_size = 3\n",
    "gamma = np.ones(hidden_size)\n",
    "beta = np.zeros(hidden_size)\n",
    "bn =BatchNormalization(gamma, beta)\n",
    "        \n",
    "x = np.array([[1,2,3],[2,3,2],[3,4,4],[4,1,2]]) # n*d行列\n",
    "print(\"入力x=\")\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print(\"学習時の順伝播計算\")\n",
    "print(bn.forward(x, train_flg=True))\n",
    "print()\n",
    "print(\"予測時の順伝播計算\")\n",
    "print(bn.forward(x, train_flg=False))\n",
    "print()\n",
    "\n",
    "print(\"勾配\")\n",
    "dout = np.array([[0.1,0.2,0.3],[0.2,0.3,0.2],[0.3,0.4,0.4],[0.4,0.1,0.2]]) \n",
    "print(dout)\n",
    "print()\n",
    "print(\"学習時の逆伝播計算\")\n",
    "print(bn.backward(dout))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力が4次元の場合\n",
    "N = 2\n",
    "C = 6\n",
    "H = 3\n",
    "W = 3\n",
    "gamma = np.ones(C)\n",
    "beta = np.zeros(C)\n",
    "bn =BatchNormalization(gamma, beta)\n",
    "        \n",
    "x = np.arange(N*C*H*W).reshape(N, C, H, W) # NCHW配列\n",
    "print(\"入力x=\")\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print(\"学習時の順伝播計算\")\n",
    "print(bn.forward(x, train_flg=True))\n",
    "print()\n",
    "print(\"予測時の順伝播計算\")\n",
    "print(bn.forward(x, train_flg=False))\n",
    "print()\n",
    "\n",
    "print(\"勾配\")\n",
    "dout = np.arange(N*C*H*W).reshape(N, C, H, W) / 10 # NCHW配列. 10は値を調整するための適当な数\n",
    "print(dout)\n",
    "print()\n",
    "print(\"学習時の逆伝播計算\")\n",
    "print(bn.backward(dout))\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
